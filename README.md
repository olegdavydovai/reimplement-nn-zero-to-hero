# Reimplement Neural Networks: Zero to Hero
I watch the playlist on YouTube [Andrej Karpathy](https://www.youtube.com/@AndrejKarpathy) - Neural Networks: Zero to hero. Then I recreate the code from the lectures from scratch and do exercises for the lectures. A course on neural networks that starts all the way at the basics. The Jupyter notebooks I build are then captured here inside the [lectures](https://github.com/olegdavydovai/reimplement-nn-zero-to-hero/tree/main/lectures) directory. [Here](https://github.com/karpathy/nn-zero-to-hero?tab=readme-ov-file) is the original GitHub of the course.

## Lecture 0: Autograd, basics of neural networks, backpropogation
I am building a Value class without libraries, from scratch in Python. It accepts data, builds an object for them and tracks the operations that go through the entire code. It knows how any value was obtained, as a result of which operations, and stores this information. Then it builds a topographic graph that tells how the code was executed from beginning to end. Due to this, back propagation is performed using chain rule differentiation. Now we know the gradients of each parameter and I update them so that the loss function decreases according to the maximum likelihood estimation rules. The loss decreases using vanilla gradient descent. I am implementing a Neuron class that accepts inputs, initializes the weights for the inputs and biases. A Layer class that consists of several independent neurons that accept the same inputs, but different weights and biases for them. An MLP class that consists of several consecutive layers.
&bull Jupyter file
