{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ed11384-ee4e-46ef-82f3-d58d22fe3185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "words = open('D:/Download/names.txt', 'r').read().splitlines()\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "\n",
    "block_size = 3\n",
    "\n",
    "def build_dataset(words):\n",
    "    X,Y = [],[]\n",
    "    for w in words:\n",
    "        context = [0]*block_size\n",
    "        for ch in w +'.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    # print (X.shape, Y.shape)\n",
    "    return X,Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])\n",
    "\n",
    "def cmp (s, dt, t):\n",
    "    ex = torch.all(dt == t.grad).item() \n",
    "    close = torch.allclose(dt, t.grad)\n",
    "    max_diff = (dt - t.grad).abs().max().item()\n",
    "    print(f'{s:15s} | exact:{str(ex)} | close:{str(close)} | max diff: {max_diff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eff39b83-f082-41dc-a5ee-7631d9fb7e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "batch_size = 32\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embd), generator = g)\n",
    "W1 = torch.randn((block_size*n_embd, n_hidden), generator = g) * (5/3)/(block_size*n_embd)**0.5\n",
    "b1 = torch.randn(n_hidden, generator = g) * 0.1\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator = g) * 0.1\n",
    "b2 = torch.randn(vocab_size, generator = g) * 0.1\n",
    "bngain = torch.randn(1, n_hidden, generator = g)*0.1 + 1.0\n",
    "bnbias = torch.randn(1, n_hidden, generator = g)*0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "print(sum(p.nelement() for p in parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7aed4dae-b575-4b78-9ca4-85c6196e7e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.9508, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = batch_size\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator = g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "emb = C[Xb]\n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "\n",
    "hprebn = embcat @ W1 + b1\n",
    "bnmeani = 1/n * hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1) * (bndiff2).sum(0, keepdim=True)\n",
    "bnvar_inv = (bnvar+1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "h = torch.tanh(hpreact)\n",
    "\n",
    "logits = h @ W2 + b2\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdim=True)\n",
    "counts_sum_inv = counts_sum**-1\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "for t in [logprobs, probs, counts_sum_inv, counts_sum, counts, norm_logits, logit_maxes, logits, \n",
    "          h, hpreact, bnraw, bnvar_inv, bnvar, bndiff2, bndiff, bnmeani, hprebn, embcat, emb]:\n",
    "    t.retain_grad()\n",
    "loss.backward()\n",
    "loss\n",
    "# loss = 3.5571"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "54797655-2831-4ea6-9e44-039ad07b802a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact:True | close:True | max diff: 0.0\n",
      "probs           | exact:True | close:True | max diff: 0.0\n",
      "counts_sum_inv  | exact:True | close:True | max diff: 0.0\n",
      "counts_sum      | exact:True | close:True | max diff: 0.0\n",
      "counts          | exact:True | close:True | max diff: 0.0\n",
      "norm_logits     | exact:True | close:True | max diff: 0.0\n",
      "logit_maxes     | exact:True | close:True | max diff: 0.0\n",
      "logits          | exact:True | close:True | max diff: 0.0\n",
      "h               | exact:True | close:True | max diff: 0.0\n",
      "W2              | exact:True | close:True | max diff: 0.0\n",
      "b2              | exact:True | close:True | max diff: 0.0\n",
      "hpreact         | exact:True | close:True | max diff: 0.0\n",
      "bngain          | exact:True | close:True | max diff: 0.0\n",
      "bnbias          | exact:True | close:True | max diff: 0.0\n",
      "bnraw           | exact:True | close:True | max diff: 0.0\n",
      "bnvar_inv       | exact:True | close:True | max diff: 0.0\n",
      "bnvar           | exact:True | close:True | max diff: 0.0\n",
      "bndiff2         | exact:True | close:True | max diff: 0.0\n",
      "bndiff          | exact:True | close:True | max diff: 0.0\n",
      "bnmeani         | exact:True | close:True | max diff: 0.0\n",
      "hprebn          | exact:True | close:True | max diff: 0.0\n",
      "embcat          | exact:True | close:True | max diff: 0.0\n",
      "W1              | exact:True | close:True | max diff: 0.0\n",
      "b1              | exact:True | close:True | max diff: 0.0\n",
      "emb             | exact:True | close:True | max diff: 0.0\n",
      "C               | exact:True | close:True | max diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n), Yb] = -1.0/n\n",
    "dprobs = dlogprobs.clone() * 1.0/probs.data.detach()\n",
    "dcounts_sum_inv = (dprobs * counts.data.detach()).sum(1, keepdim = True)\n",
    "dcounts = dprobs * counts_sum_inv\n",
    "dcounts_sum = dcounts_sum_inv * -counts_sum**-2\n",
    "dcounts += dcounts_sum * torch.ones_like(counts)\n",
    "dnorm_logits = dcounts * norm_logits.exp()\n",
    "dlogit_maxes = -dnorm_logits.sum(1, keepdim=True)\n",
    "dlogits = dnorm_logits.clone()\n",
    "dextra = torch.zeros_like(logits)\n",
    "dextra[range(n), logits.max(1).indices] = 1\n",
    "dlogits += dlogit_maxes * dextra\n",
    "# dlogits += F.one_hot(logits.max(1).indices, num_classes = logits.shape[1])*dlogit_maxes\n",
    "dh = dlogits @ W2.T\n",
    "dW2 = h.T @ dlogits\n",
    "db2 = dlogits.sum(0)\n",
    "dhpreact = dh * (1.0 - h**2)\n",
    "dbngain = (dhpreact * bnraw).sum(0, keepdim=True)\n",
    "dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "dbnraw = dhpreact * bngain\n",
    "dbndiff = dbnraw * bnvar_inv\n",
    "dbnvar_inv = (dbnraw * bndiff).sum(0, keepdim= True)\n",
    "dbnvar = dbnvar_inv * (-0.5 * (bnvar+1e-5)**-1.5)\n",
    "dbndiff2 = dbnvar * torch.ones_like(bndiff2) * (1.0/(n-1))\n",
    "dbndiff += dbndiff2 * 2 * bndiff\n",
    "dbnmeani = -dbndiff.sum(0, keepdim = True)\n",
    "dhprebn = dbndiff.clone()\n",
    "dhprebn += dbnmeani * torch.ones_like(hprebn) / n\n",
    "dembcat = dhprebn @ W1.T\n",
    "dW1 = embcat.T @ dhprebn\n",
    "db1 = dhprebn.sum(0)\n",
    "demb = dembcat.clone().view(emb.shape)\n",
    "dC = torch.zeros_like(C)\n",
    "for i in range (Xb.shape[0]):\n",
    "    for j in range (Xb.shape[1]):\n",
    "        x = Xb[i, j]\n",
    "        dC[x] += demb[i,j]\n",
    "\n",
    "\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)\n",
    "cmp('emb', demb, emb)\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ffb3db37-fff6-4768-95fe-71fbe242ebdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16059\n"
     ]
    }
   ],
   "source": [
    "n_embd = 16\n",
    "n_hidden = 200\n",
    "batch_size = 64\n",
    "n = batch_size\n",
    "max_steps = 200001\n",
    "lr = 0.1\n",
    "lr_decay = 0.03\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embd), generator = g)\n",
    "W1 = torch.randn((block_size*n_embd, n_hidden), generator = g) * (5/3)/(block_size*n_embd)**0.5\n",
    "b1 = torch.randn(n_hidden, generator = g) * 0.1\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator = g) * 0.1\n",
    "b2 = torch.randn(vocab_size, generator = g) * 0.1\n",
    "bngain = torch.randn(1, n_hidden, generator = g)*0.1 + 1.0\n",
    "bnbias = torch.randn(1, n_hidden, generator = g)*0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "print(sum(p.nelement() for p in parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27c6926c-e633-4016-80af-4506bbf40da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(1.9306)\n",
      "10000 tensor(1.9482)\n",
      "20000 tensor(2.0499)\n",
      "30000 tensor(1.8415)\n",
      "40000 tensor(1.8654)\n",
      "50000 tensor(2.4896)\n",
      "60000 tensor(2.1975)\n",
      "70000 tensor(1.9699)\n",
      "80000 tensor(2.0600)\n",
      "90000 tensor(1.8972)\n",
      "100000 tensor(2.1812)\n",
      "110000 tensor(1.7568)\n",
      "120000 tensor(1.9225)\n",
      "130000 tensor(2.2120)\n",
      "140000 tensor(2.1268)\n",
      "150000 tensor(1.9954)\n",
      "160000 tensor(2.2609)\n",
      "170000 tensor(1.6453)\n",
      "180000 tensor(1.9065)\n",
      "190000 tensor(2.0167)\n",
      "200000 tensor(1.9223)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i in range (max_steps):\n",
    "        ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator = g)\n",
    "        Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "        \n",
    "        emb = C[Xb]\n",
    "        embcat = emb.view(emb.shape[0], -1)\n",
    "        \n",
    "        hprebn = embcat @ W1 + b1\n",
    "        \n",
    "        bnmean = hprebn.mean(0, keepdim=True)\n",
    "        bnvar = hprebn.var(0, keepdim=True)\n",
    "        bnvar_inv = (bnvar+1e-5)**-0.5\n",
    "        bnraw = (hprebn-bnmean) * bnvar_inv\n",
    "        hpreact = bngain * bnraw + bnbias\n",
    "        \n",
    "        h = torch.tanh(hpreact)\n",
    "        logits = h @ W2 + b2\n",
    "        loss = F.cross_entropy(logits, Yb)\n",
    "        \n",
    "        # for p in parameters:\n",
    "        #     p.grad = None\n",
    "        # loss.backward()\n",
    "        \n",
    "        dlogits = F.softmax(logits, 1)\n",
    "        dlogits[range(n), Yb] -= 1\n",
    "        dlogits/=n\n",
    "        dh = dlogits @ W2.T\n",
    "        dW2 = h.T @ dlogits\n",
    "        db2 = dlogits.sum(0)\n",
    "        dhpreact = dh * (1-h**2)\n",
    "        dbngain = (dhpreact * bnraw).sum(0, keepdim= True)\n",
    "        dbnbias = (dhpreact).sum(0, keepdim= True)\n",
    "        dhprebn = bngain * bnvar_inv / n * (n*dhpreact - dhpreact.sum(0) - n/(n-1) * bnraw * (bnraw * dhpreact).sum(0))\n",
    "        dembcat = dhprebn @ W1.T\n",
    "        dW1 = embcat.T @ dhprebn\n",
    "        db1 = dhprebn.sum(0)\n",
    "        demb = dembcat.view(emb.shape)\n",
    "        dC = torch.zeros_like(C)\n",
    "        for j in range (emb.shape[0]):\n",
    "            for k in range (emb.shape[1]):\n",
    "                x = Xb[j,k]\n",
    "                dC[x] += demb[j,k]\n",
    "    \n",
    "        grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "    \n",
    "        lr = lr if i<(max_steps//2) else lr_decay\n",
    "        for p, grad in zip(parameters, grads):\n",
    "            p.data += - lr * grad\n",
    "            # p.data += - lr * p.grad\n",
    "        if i%10000 == 0:\n",
    "            print (i, loss)\n",
    "        # if i==100:\n",
    "        #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eb9dee69-e257-4885-b926-d65baaafeffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 16)        | exact:False | close:True | max diff: 1.4901161193847656e-08\n",
      "(48, 200)       | exact:False | close:True | max diff: 1.1175870895385742e-08\n",
      "(200,)          | exact:False | close:True | max diff: 3.259629011154175e-09\n",
      "(200, 27)       | exact:False | close:True | max diff: 1.4901161193847656e-08\n",
      "(27,)           | exact:False | close:True | max diff: 7.450580596923828e-09\n",
      "(1, 200)        | exact:False | close:True | max diff: 2.7939677238464355e-09\n",
      "(1, 200)        | exact:False | close:True | max diff: 2.7939677238464355e-09\n"
     ]
    }
   ],
   "source": [
    "for p,g in zip(parameters, grads):\n",
    "    cmp(str(tuple(p.shape)), g, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "289fdc6f-4fd9-4e85-93c6-4552cb379326",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    emb = C[Xtr]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hprebn = embcat @ W1 + b1\n",
    "    bnmean = hprebn.mean(0, keepdim = True)\n",
    "    bnvar = hprebn.var(0, keepdim = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e5661d0-d93d-4fdd-a900-d4d4ede47471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.019171714782715\n",
      "val 2.08778715133667\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    x, y = {\n",
    "        'train':(Xtr,Ytr),\n",
    "        'val':(Xdev,Ydev),\n",
    "        'test':(Xte,Yte)\n",
    "    }[split]\n",
    "    emb = C[x]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hprebn = embcat @ W1 + b1\n",
    "    hpreact = bngain*(hprebn - bnmean)/ (bnvar+1e-5)**0.5 + bnbias\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 +b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "931335f8-6c7c-4430-9e76-4b6ba71b252d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chris.\n",
      "quan.\n",
      "maximark.\n",
      "fhampplivitta.\n",
      "mckendram.\n",
      "qua.\n",
      "jamiyah.\n",
      "jaxstyn.\n",
      "tri.\n",
      "mckiella.\n",
      "jakelsey.\n",
      "jakodaktusakau.\n",
      "jammierrick.\n",
      "sperson.\n",
      "gwennalupudnson.\n",
      "blaksh.\n",
      "pynn.\n",
      "branvik.\n",
      "fenne.\n",
      "fram.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "for _ in range(20):\n",
    "    out=[]\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])]\n",
    "        embcat = emb.view(emb.shape[0], -1)\n",
    "        hprebn = embcat @ W1 + b1 \n",
    "        hpreact = bngain * (hprebn-bnmean)/(bnvar+1e-5)**-0.5 + bnbias\n",
    "        h = torch.tanh(hpreact)\n",
    "        logits = h @ W2 + b2\n",
    "        \n",
    "        probs = F.softmax (logits, 1)\n",
    "        ix = torch.multinomial(probs, 1, generator = g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c5c4501-1585-4c05-9d7b-7a1ae0770d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 2.0899453163146973\n"
     ]
    }
   ],
   "source": [
    "split_loss('test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
